{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "import pandas as pd\n",
    "from google.oauth2.service_account import Credentials\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('setting.json') as f:\n",
    "    df = json.load(f)\n",
    "    SPREADSHEET_KEY = df['KANAGAWA_SPREADSHEET_KEY']\n",
    "    SECRET_KEY_PATH = df['SECRET_KEY_PATH']\n",
    "    \n",
    "scope = ['https://www.googleapis.com/auth/spreadsheets',\n",
    "            'https://www.googleapis.com/auth/drive']\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    SECRET_KEY_PATH, scopes=scope)\n",
    "gc = gspread.authorize(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#スプレッドシートキーを指定してワークブックを選択\n",
    "workbook = gc.open_by_key(SPREADSHEET_KEY).sheet1\n",
    "shop_name_list = workbook.col_values(1)\n",
    "shop_url_list = workbook.col_values(2)\n",
    "\n",
    "#df_shopをcsvに\n",
    "df_shop = pd.DataFrame([shop_name_list, shop_url_list], index=['name', 'url']).T\n",
    "df_shift = df_shop.shift(periods=-1,axis=0)\n",
    "df_shift.to_csv('shop_list.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csvfileの読み込み\n",
    "df_shift = pd.read_csv('shop_list.csv')\n",
    "df_array = []\n",
    "for i in range(len(shop_name_list)-1):\n",
    "    #if i ==3:\n",
    "        #break\n",
    "    url = df_shift.at[i, 'url']\n",
    "#url = df_shop.at[1, 'url']\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    s = soup.find_all(class_ ='w620')\n",
    "    try:\n",
    "        tel_url = s[0].find('a').get('href')\n",
    "\n",
    "    except(AttributeError):\n",
    "        tel_url = None\n",
    "        tel = None\n",
    "        \n",
    "    else:\n",
    "        tel_r = requests.get(tel_url)\n",
    "        tel_soup = BeautifulSoup(tel_r.text)\n",
    "        tel = tel_soup.find(class_='fs16').text\n",
    "        \n",
    "\n",
    "    try:\n",
    "        other = s[2].text+\" \"+s[5].text+\" \"+s[6].text+\" \"+s[8].text+\" \"+s[9].text\n",
    "    \n",
    "    except(IndexError):\n",
    "        other = s[2].text+\" \"+s[5].text+\" \"+s[6].text+\" \"+s[8].text\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'url': [url], \n",
    "        '住所' : [s[1].text], \n",
    "        '営業時間' :[s[3].text], \n",
    "        '定休日':[s[4].text], \n",
    "        '設備':[s[6].text], \n",
    "        'スタッフ数' :[s[7].text], \n",
    "        '備考':[other] , \n",
    "        'TEL_URL':[tel_url], \n",
    "        'TEL':[tel],\n",
    "        })\n",
    "    #df = pd.DataFrame([url, s[1].text, s[3].text, s[4].text, s[6].text, s[7].text, other, tel_url, tel], index = ['url', '住所', '営業時間', '定休日', '設備', 'スタッフ数', '備考', 'TEL_URL', 'TEL'])\n",
    "    \n",
    "    df_merge= df_shift.merge(df, on='url')\n",
    "    df_array.append(df_merge)\n",
    "df_concat = pd.concat(df_array).reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.to_csv('esthetic.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectname",
   "language": "python",
   "name": "projectname"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
